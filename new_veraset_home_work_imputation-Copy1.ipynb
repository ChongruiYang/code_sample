{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Home and Work Locations\n",
    "## Veraset Data - 05/2019 & 06/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date: 3/20/2022\n",
    "# author: Ziwen\n",
    "# task: identify home and work location with Veraset rawpings\n",
    "import os, sys, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "import ipystata\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "path = os.path.expanduser('~')\n",
    "# set up env credential variable\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path + '/Dropbox/Amenity/source/analysis/veraset_gravy_gps_sample/firm-exit-3608acd14b06.json'\n",
    "client = bigquery.Client()\n",
    "\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from GCS to BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute home and work locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x20b7fee79a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label overnight and weekday-daytime records\n",
    "default_dataset = 'firm-exit.veraset_visits'\n",
    "destination_table = 'firm-exit.veraset_visits.time_converted_raw_ping_week'\n",
    "job_config = bigquery.QueryJobConfig(write_disposition='WRITE_TRUNCATE', # overwrite\n",
    "                                     destination=destination_table,\n",
    "                                     default_dataset = default_dataset)\n",
    "\n",
    "query_label = f'''\n",
    "    with time_converted_table as\n",
    "    (select \n",
    "        -- take necessary variables and conversions\n",
    "        caid as caid, \n",
    "        datetime_sub(timestamp_seconds(utc_timestamp), interval 7 hour) as PDT_time,\n",
    "        extract(hour from datetime_sub(timestamp_seconds(utc_timestamp), interval 7 hour)) as hofd,\n",
    "        format_datetime('%A', datetime_sub(timestamp_seconds(utc_timestamp), interval 7 hour)) as weekday,\n",
    "        format_datetime('%r', datetime_sub(timestamp_seconds(utc_timestamp), interval 7 hour)) as time_of_day,\n",
    "        substr(geo_hash, 1, 7) as geohash7,\n",
    "        latitude as latitude,\n",
    "        longitude as longitude\n",
    "    from `firm-exit.veraset_visits.new_raw_ping`), week_dummy as\n",
    "    -- generate a week dummy\n",
    "    (select time_converted_table.*, \n",
    "        case \n",
    "            -- keep away from the April 30th to May 5th\n",
    "            when (PDT_time < '2019-05-06 00:00:00') then '0'\n",
    "            when (PDT_time >= '2019-05-06 00:00:00') and (PDT_time < '2019-05-13 00:00:00') then '1'\n",
    "            when (PDT_time >= '2019-05-13 00:00:00') and (PDT_time < '2019-05-20 00:00:00') then '2'\n",
    "            when (PDT_time >= '2019-05-20 00:00:00') and (PDT_time < '2019-05-27 00:00:00') then '3'\n",
    "            when (PDT_time >= '2019-05-27 00:00:00') and (PDT_time < '2019-06-03 00:00:00') then '4'\n",
    "            when (PDT_time >= '2019-06-03 00:00:00') and (PDT_time < '2019-06-10 00:00:00') then '5'\n",
    "            when (PDT_time >= '2019-06-10 00:00:00') and (PDT_time < '2019-06-17 00:00:00') then '6'\n",
    "            when (PDT_time >= '2019-06-17 00:00:00') and (PDT_time < '2019-06-24 00:00:00') then '7'\n",
    "            when (PDT_time >= '2019-06-24 00:00:00') and (PDT_time < '2019-07-01 00:00:00') then '8'\n",
    "            else NULL\n",
    "        end\n",
    "        as week\n",
    "    from time_converted_table), day as\n",
    "    (select week_dummy.*, \n",
    "        case \n",
    "            -- overnight indicator: 7pm to 7am of the next day\n",
    "            when (hofd >= 19 and hofd <= 23) or (hofd >= 0 and hofd < 7) then 'overnight'\n",
    "            -- weekday daytime indicator: 8am to 6pm from mon to fri\n",
    "            when (weekday = 'Monday' or weekday = 'Tuesday' or weekday = 'Wednesday' or weekday = 'Thursday' or weekday = 'Friday') and (hofd >= 8) and (hofd < 18) then 'weekday_daytime'\n",
    "            else NULL\n",
    "        end\n",
    "        as time_indicator\n",
    "    from week_dummy\n",
    "    )\n",
    "    select day.*,\n",
    "    --calculate a total num_overnight_pings first\n",
    "        countif(time_indicator = 'overnight') OVER (PARTITION BY day.caid, day.geohash7) as num_records_overnight, \n",
    "        countif(time_indicator = 'weekday_daytime') OVER (PARTITION BY day.caid, day.geohash7) as num_records_weekday_daytime,\n",
    "        avg(hofd) OVER (PARTITION BY caid, geohash7) as avg_hofd,\n",
    "        avg(latitude) OVER (PARTITION BY caid, geohash7) as avg_lat, \n",
    "        avg(longitude) OVER (PARTITION BY caid, geohash7) as avg_lng,\n",
    "    from day\n",
    "'''\n",
    "\n",
    "query_job = client.query(query_label, job_config=job_config)\n",
    "query_job.result() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x20b7ff37b80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of records at each geohash7 region, and take the average of lat & lng\n",
    "# take only the most common overnight location as a candidate for home\n",
    "# and take the first and second most common weekday daytime location as candidates for work\n",
    "destination_table = 'firm-exit.veraset_visits.time_converted_raw_ping_week'\n",
    "job_config = bigquery.QueryJobConfig(write_disposition='WRITE_TRUNCATE', # overwrite\n",
    "                                     destination=destination_table,\n",
    "                                     default_dataset = default_dataset)\n",
    "\n",
    "\n",
    "query_group = f'''\n",
    "    with grouped_raw_ping as\n",
    "    -- collapse data to week-device-geohash7 region,select the weekly raw_ping for first round selection\n",
    "    (select \n",
    "        num_records_overnight as num_records_overnight,\n",
    "        num_records_weekday_daytime as num_records_weekday_daytime,\n",
    "        avg_hofd as avg_hofd,\n",
    "        caid as caid, \n",
    "        countif(time_indicator = 'overnight') OVER (PARTITION BY caid, geohash7,week) as num_records_overnight_week, \n",
    "        countif(time_indicator = 'weekday_daytime') OVER (PARTITION BY caid, geohash7,week) as num_records_weekday_daytime_week,\n",
    "        week as week,\n",
    "        avg_lat as avg_lat,\n",
    "        avg_lng as avg_lng,\n",
    "        avg(hofd) OVER (PARTITION BY caid, geohash7,week) as avg_hofd_week, \n",
    "        geohash7 as geohash7\n",
    "    from `firm-exit.veraset_visits.time_converted_raw_ping_week`\n",
    "    ), ranked_raw_ping as\n",
    "    -- rank counted number of records by weeks (there could be ties)\n",
    "    (select\n",
    "        grouped_raw_ping.*,\n",
    "        dense_rank() over (partition by grouped_raw_ping.caid,grouped_raw_ping.week order by grouped_raw_ping.num_records_overnight_week DESC) as rank_home_week,\n",
    "        dense_rank() over (partition by grouped_raw_ping.caid,grouped_raw_ping.week order by grouped_raw_ping.num_records_weekday_daytime_week DESC) as rank_work_week\n",
    "     from grouped_raw_ping\n",
    "    )\n",
    "    select \n",
    "        *\n",
    "    from ranked_raw_ping\n",
    "    -- keep only top ranked home location and top and second ranked work location (in case top work is already identified as home)\n",
    "    where (rank_home_week = 1 and num_records_overnight != 0) or (rank_work_week = 1 and num_records_weekday_daytime !=0) or (rank_work_week = 2 and num_records_weekday_daytime !=0)\n",
    "    order by caid\n",
    "'''\n",
    "\n",
    "\n",
    "query_job = client.query(query_group, job_config=job_config)\n",
    "query_job.result() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x20b7feb9b80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_table = 'firm-exit.veraset_visits.time_converted_raw_ping_week'\n",
    "job_config = bigquery.QueryJobConfig(write_disposition='WRITE_TRUNCATE', # overwrite\n",
    "                                     destination=destination_table,\n",
    "                                     default_dataset = default_dataset)\n",
    "query_group = f'''\n",
    "    select distinct*\n",
    "    FROM `firm-exit.veraset_visits.time_converted_raw_ping_week`\n",
    "'''\n",
    "\n",
    "query_job = client.query(query_group, job_config=job_config)\n",
    "query_job.result() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported firm-exit:veraset_visits.time_converted_raw_ping_week to gs://veraset_temp/home_work_week_candidates*.csv\n"
     ]
    }
   ],
   "source": [
    "# obtain data from gcs\n",
    "# export to gcs and download\n",
    "project = \"firm-exit\"\n",
    "dataset_id = \"veraset_visits\"\n",
    "table_id = \"time_converted_raw_ping_week\"\n",
    "\n",
    "destination_uri = \"gs://{}/{}\".format('veraset_temp', \"home_work_week_candidates*.csv\")\n",
    "dataset_ref = bigquery.DatasetReference(project, dataset_id)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location=\"US\",\n",
    ")  # API request\n",
    "extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "print(\n",
    "    \"Exported {}:{}.{} to {}\".format(project, dataset_id, table_id, destination_uri)\n",
    ")\n",
    "\n",
    "# download data from gcs to our dropbox, first download 1 - 10 and then 10 - 43\n",
    "file_index = [i for i in range(17,26)]\n",
    "\n",
    "for x in file_index:\n",
    "    # Initialise a client\n",
    "    storage_client = storage.Client(\"firm-exit\")\n",
    "    # Create a bucket object for our bucket\n",
    "    bucket = storage_client.get_bucket(\"veraset_temp\")\n",
    "    # Create a blob object from the filepath\n",
    "    blob = bucket.blob(\"home_work_week_candidates0000000000{}.csv\".format(str(x)))\n",
    "    # Download the file to a destination\n",
    "    blob.download_to_filename(path + '/Dropbox/Amenity/data/derived/veraset_gravy_gps_sample/veraset/home_work_week_candidates_{}.csv'.format(str(x)))\n",
    "    # Delete after exporting, otherwise costs storage fees\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download to local and run stata then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caid</th>\n",
       "      <th>num_records_overnight</th>\n",
       "      <th>num_records_weekday_daytime</th>\n",
       "      <th>avg_lat</th>\n",
       "      <th>avg_lng</th>\n",
       "      <th>avg_hofd</th>\n",
       "      <th>geohash7</th>\n",
       "      <th>rank_home</th>\n",
       "      <th>rank_work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000025006594b60185fd44d02aa45689a6c63e47b7037...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>34.061700</td>\n",
       "      <td>-118.400051</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>9q5cc8x</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000025006594b60185fd44d02aa45689a6c63e47b7037...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>33.790800</td>\n",
       "      <td>-118.324800</td>\n",
       "      <td>19.818182</td>\n",
       "      <td>9q5b5rq</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000488029787044d3751df4d476a7a0ea1c5b1e8bb66...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.963962</td>\n",
       "      <td>-118.334930</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9q5c5nz</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000488029787044d3751df4d476a7a0ea1c5b1e8bb66...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>34.045853</td>\n",
       "      <td>-118.321730</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>9q5cetb</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000096512b9cce231abdec8d6ea460d2e750123161cb5...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34.686090</td>\n",
       "      <td>-120.436830</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>9q4m76k</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                caid  num_records_overnight  \\\n",
       "0  0000025006594b60185fd44d02aa45689a6c63e47b7037...                      0   \n",
       "1  0000025006594b60185fd44d02aa45689a6c63e47b7037...                     30   \n",
       "2  00000488029787044d3751df4d476a7a0ea1c5b1e8bb66...                      0   \n",
       "3  00000488029787044d3751df4d476a7a0ea1c5b1e8bb66...                     10   \n",
       "4  0000096512b9cce231abdec8d6ea460d2e750123161cb5...                      1   \n",
       "\n",
       "   num_records_weekday_daytime    avg_lat     avg_lng   avg_hofd geohash7  \\\n",
       "0                            2  34.061700 -118.400051  16.500000  9q5cc8x   \n",
       "1                            0  33.790800 -118.324800  19.818182  9q5b5rq   \n",
       "2                            1  33.963962 -118.334930  13.000000  9q5c5nz   \n",
       "3                            0  34.045853 -118.321730  10.133333  9q5cetb   \n",
       "4                            2  34.686090 -120.436830  15.666667  9q4m76k   \n",
       "\n",
       "   rank_home  rank_work  \n",
       "0         10          1  \n",
       "1          1          2  \n",
       "2          2          1  \n",
       "3          1          2  \n",
       "4          2          2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import home_work_candidates\n",
    "read_path = path + '/Dropbox/Amenity/data/derived/veraset_gravy_gps_sample/veraset'\n",
    "all_files = glob.glob(read_path + \"/home_work_candidates_*.csv\")\n",
    "\n",
    "temp = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    temp.append(df)\n",
    "\n",
    "home_work_candidates = pd.concat(temp, axis=0, ignore_index=True)\n",
    "home_work_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caid                           6616967\n",
       "num_records_overnight          6616967\n",
       "num_records_weekday_daytime    6616967\n",
       "avg_lat                        6616967\n",
       "avg_lng                        6616967\n",
       "avg_hofd                       6616967\n",
       "geohash7                       6616967\n",
       "rank_home                      6616967\n",
       "rank_work                      6616967\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6060053"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "caid                           12647428\n",
       "num_records_overnight          12647428\n",
       "num_records_weekday_daytime    12647428\n",
       "avg_lat                        12647428\n",
       "avg_lng                        12647428\n",
       "avg_hofd                       12647428\n",
       "geohash7                       12647428\n",
       "rank_home                      12647428\n",
       "rank_work                      12647428\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5761051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take home candidates from home_work_candidates\n",
    "# there could be multiple home location candidates for a single device\n",
    "home_candidates = home_work_candidates[(home_work_candidates['rank_home']==1) & (home_work_candidates['num_records_overnight']!=0)]\n",
    "display(home_candidates.count(), home_candidates.caid.nunique())\n",
    "\n",
    "# take work candidates from home_work_candidates\n",
    "work_candidates = home_work_candidates[(home_work_candidates['rank_work'] <= 2) & (home_work_candidates['num_records_weekday_daytime']!=0)]\n",
    "display(work_candidates.count(), work_candidates.caid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_candidates = home_candidates.assign(ties = home_candidates.duplicated(subset=['caid'], keep=False)) # mark ties\n",
    "\n",
    "# for ties, take the one with avg_hofd closest to 1am (mid of 7pm to 7am overnight window)\n",
    "# not the median/mean hofd of all home candidates because the mean/median is in daytime\n",
    "home_candidates['dev_1am'] = home_candidates['avg_hofd'].apply(lambda x: abs(x-1) if x <= 13 else abs(25-x))\n",
    "home_candidates['rank_ties'] = home_candidates[home_candidates['ties']==True].groupby('caid')['dev_1am'].rank(method=\"min\", ascending=True)\n",
    "\n",
    "home_candidates = home_candidates[(home_candidates['rank_ties']==1)|(home_candidates['ties']==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6060053"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for remaining ties, no reason to prefer one to another, select any [only a very small proportion]\n",
    "home_candidates['still_ties']=home_candidates.duplicated(subset=['caid'], keep=False) # mark remaining ties\n",
    "home_candidates = home_candidates.drop_duplicates(subset=['caid'], keep='first')\n",
    "\n",
    "display(home_candidates.caid.nunique())\n",
    "home_locations = home_candidates\n",
    "\n",
    "home_locations.to_csv(path+'/Dropbox/Amenity/data/analysis/veraset_gravy_gps_sample/new_veraset_home_locations.csv', sep=',', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join identified home locations to work candidates, and remove these already identified home locations\n",
    "identified_homes = home_locations[['caid', 'geohash7']]\n",
    "identified_homes.columns = ['caid','home_geohash7']\n",
    "work_candidates=work_candidates.join(identified_homes.set_index('caid'), on='caid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of work candidates that are ranked top, second\n",
    "num_top = work_candidates[work_candidates['rank_work']==1].groupby('caid')['caid'].count().to_frame('num_top')\n",
    "work_candidates = work_candidates.join(num_top, on='caid')\n",
    "\n",
    "num_second = work_candidates[work_candidates['rank_work']==2].groupby('caid')['caid'].count().to_frame('num_second')\n",
    "work_candidates = work_candidates.join(num_second, on='caid')\n",
    "work_candidates['num_second'] = work_candidates['num_second'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow down the candidate pool a bit, still allowing ties\n",
    "# remove home locations, except for when there's no other back-up work locations\n",
    "work_candidates = work_candidates[(work_candidates['geohash7']!=work_candidates['home_geohash7']) \n",
    "                                  | ((work_candidates['geohash7']==work_candidates['home_geohash7']) & (work_candidates['num_top']==1) & (work_candidates['num_second']==0))]\n",
    "\n",
    "# now that work candidates no more overlap with home locations (except for potential WFH)\n",
    "# let's re-rank work candidates and keep only new top ranked work candidates\n",
    "work_candidates['new_rank_work'] = work_candidates.groupby('caid')['num_records_weekday_daytime'].rank(method=\"min\",ascending=False)\n",
    "work_candidates = work_candidates[work_candidates['new_rank_work']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same done with home candidates to select among ties\n",
    "work_candidates['ties'] = work_candidates.duplicated(subset=['caid'], keep=False) # mark ties\n",
    "\n",
    "# for ties, take the one with avg_hofd closest to 1pm (mid of 8am to 6pm overnight window)\n",
    "# very close to median/mean hofd of all work candidates\n",
    "work_candidates['dev_1pm'] = work_candidates['avg_hofd'].apply(lambda x: abs(x-13))\n",
    "work_candidates['rank_ties'] = work_candidates[work_candidates['ties']==True].groupby('caid')['dev_1pm'].rank(method=\"min\", ascending=True)\n",
    "\n",
    "work_candidates = work_candidates[(work_candidates['rank_ties']==1)|(work_candidates['ties']==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5761051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "caid                           210012\n",
       "num_records_overnight          210012\n",
       "num_records_weekday_daytime    210012\n",
       "avg_lat                        210012\n",
       "avg_lng                        210012\n",
       "avg_hofd                       210012\n",
       "geohash7                       210012\n",
       "rank_home                      210012\n",
       "rank_work                      210012\n",
       "home_geohash7                  192066\n",
       "num_top                        210012\n",
       "num_second                     210012\n",
       "new_rank_work                  210012\n",
       "ties                           210012\n",
       "dev_1pm                        210012\n",
       "rank_ties                      210012\n",
       "still_ties                     210012\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for remaining ties, no reason to prefer one to another, select any [only a very small proportion]\n",
    "work_candidates['still_ties']=work_candidates.duplicated(subset=['caid'], keep=False) # mark remaining ties\n",
    "display(work_candidates.caid.nunique(), work_candidates[work_candidates['still_ties']==True].count())\n",
    "\n",
    "work_candidates = work_candidates.drop_duplicates(subset=['caid'], keep='first')\n",
    "work_locations = work_candidates\n",
    "work_locations.to_csv(path+'/Dropbox/Amenity/data/analysis/veraset_gravy_gps_sample/new_veraset_work_locations.csv', sep=',', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caid                           424989\n",
       "num_records_overnight          424989\n",
       "num_records_weekday_daytime    424989\n",
       "avg_lat                        424989\n",
       "avg_lng                        424989\n",
       "avg_hofd                       424989\n",
       "geohash7                       424989\n",
       "rank_home                      424989\n",
       "rank_work                      424989\n",
       "home_geohash7                  424989\n",
       "num_top                        424989\n",
       "num_second                     424989\n",
       "new_rank_work                  424989\n",
       "ties                           424989\n",
       "dev_1pm                        424989\n",
       "rank_ties                           0\n",
       "still_ties                     424989\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# potential WFH, i.e., the home location is the same as the work location\n",
    "work_candidates[work_candidates['geohash7']==work_candidates['home_geohash7']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of unique IDs in raw ping data that have have been to at least one non-home & non-work POI in CA is 6566991\n",
      "The percentage of home locations identified is 92.28%\n",
      "The percentage of work locations identified is 87.73%\n",
      "The percentage of both home & work locations identified is 81.80%\n"
     ]
    }
   ],
   "source": [
    "num_devices = 6566991 # number of unique devices from raw ping in ca\n",
    "num_home_identified = home_locations.caid.nunique() # exactly the number of devices in home_work_candidates\n",
    "num_work_identified = work_locations.caid.nunique() # exactly the number of devices in home_work_candidates\n",
    "percent_home = \"{:.2%}\".format(num_home_identified/num_devices)\n",
    "percent_work = \"{:.2%}\".format(num_work_identified/num_devices)\n",
    "\n",
    "# for those we can identify both their home and work locations\n",
    "home_work_locations = home_locations.join(work_locations.set_index('caid'), on='caid', how='inner', lsuffix='_home', rsuffix='_work')\n",
    "num_home_work_identified = home_work_locations.caid.nunique()\n",
    "percent_home_work = \"{:.2%}\".format(num_home_work_identified/num_devices) \n",
    "\n",
    "print(\"The total number of unique IDs in raw ping data that have have been to at least one non-home & non-work POI in CA is {}\".format(num_devices))\n",
    "print(\"The percentage of home locations identified is {}\".format(percent_home))\n",
    "print(\"The percentage of work locations identified is {}\".format(percent_work))\n",
    "print(\"The percentage of both home & work locations identified is {}\".format(percent_home_work))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
